{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPsVqXd/Ed4G0Yj3HXESo66",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathvikapolepelly/REINFORCEMENT-LEARNING/blob/main/Assigmnent%204.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "vRQ7kvJlt-Fp"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "from typing import Callable, Tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def epsilon_greedy(Q: np.ndarray, s: int, epsilon: float) -> int:\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(Q.shape[1])\n",
        "    return int(np.argmax(Q[s]))\n"
      ],
      "metadata": {
        "id": "9X4nEDRUuMid"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_episode(env, policy: Callable[[int], int], gamma: float = 0.99, max_steps: int = 1000) -> Tuple[float, int]:\n",
        "    s, _ = env.reset()\n",
        "    total_reward, steps = 0.0, 0\n",
        "    for _ in range(max_steps):\n",
        "        a = policy(s)\n",
        "        s_next, r, terminated, truncated, _ = env.step(a)\n",
        "        total_reward += r * (gamma ** steps)\n",
        "        steps += 1\n",
        "        s = s_next\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    return total_reward, steps\n",
        "\n"
      ],
      "metadata": {
        "id": "CWAFs-davrSB"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def td0_policy_evaluation(env_id: str = \"FrozenLake-v1\",\n",
        "                          is_slippery: bool = True,\n",
        "                          gamma: float = 0.99,\n",
        "                          alpha: float = 0.1,\n",
        "                          epsilon_random_policy: float = 1.0,\n",
        "                          episodes: int = 20_000,\n",
        "                          seed: int = 0) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Estimates V^pi for a fixed policy using TD(0).\n",
        "    By default, pi is a fully random policy (epsilon_random_policy = 1.0).\n",
        "    \"\"\"\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    rng = np.random.default_rng(seed)\n",
        "    env.reset(seed=seed)\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    V = np.zeros(nS, dtype=np.float64)\n",
        "\n",
        "    def policy(s: int) -> int:\n",
        "        # Equiprobable random (or epsilon-random around greedy on V, but typically random)\n",
        "        if rng.random() < epsilon_random_policy:\n",
        "            return rng.integers(nA)\n",
        "        # If not fully random, do a crude one-step lookahead using V (optional)\n",
        "        return rng.integers(nA)\n",
        "\n",
        "    for ep in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            a = policy(s)\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            td_target = r + (0.0 if (terminated or truncated) else gamma * V[s_next])\n",
        "            V[s] += alpha * (td_target - V[s])\n",
        "            s = s_next\n",
        "            done = terminated or truncated\n",
        "\n",
        "    env.close()\n",
        "    return V\n",
        "\n",
        "# ---------------------------\n",
        "# SARSA (on-policy TD control)\n",
        "# ---------------------------\n",
        "\n",
        "def sarsa_control(env_id: str = \"FrozenLake-v1\",\n",
        "                  is_slippery: bool = True,\n",
        "                  gamma: float = 0.99,\n",
        "                  alpha: float = 0.1,\n",
        "                  epsilon_start: float = 1.0,\n",
        "                  epsilon_end: float = 0.05,\n",
        "                  epsilon_decay_steps: int = 50_000,\n",
        "                  episodes: int = 100_000,\n",
        "                  max_steps_per_ep: int = 200,\n",
        "                  seed: int = 1) -> Tuple[np.ndarray, np.ndarray]:\n",
        "    \"\"\"\n",
        "    Learns an epsilon-greedy policy for Q using on-policy SARSA.\n",
        "    Returns learned Q and the derived greedy policy (as a 1D array of actions).\n",
        "    \"\"\"\n",
        "    env = gym.make(env_id, is_slippery=is_slippery)\n",
        "    env.reset(seed=seed)\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    nS = env.observation_space.n\n",
        "    nA = env.action_space.n\n",
        "    Q = np.zeros((nS, nA), dtype=np.float64)\n",
        "\n",
        "    def epsilon_by_step(t: int) -> float:\n",
        "        # Linear decay\n",
        "        frac = min(1.0, max(0.0, t / max(1, epsilon_decay_steps)))\n",
        "        return epsilon_start + (epsilon_end - epsilon_start) * frac\n",
        "\n",
        "    timestep = 0\n",
        "    for ep in range(episodes):\n",
        "        s, _ = env.reset()\n",
        "        eps = epsilon_by_step(timestep)\n",
        "        a = rng.integers(nA) if rng.random() < eps else int(np.argmax(Q[s]))\n",
        "        for _ in range(max_steps_per_ep):\n",
        "            s_next, r, terminated, truncated, _ = env.step(a)\n",
        "            eps_next = epsilon_by_step(timestep + 1)\n",
        "            a_next = rng.integers(nA) if rng.random() < eps_next else int(np.argmax(Q[s_next]))\n",
        "            td_target = r + (0.0 if (terminated or truncated) else gamma * Q[s_next, a_next])\n",
        "            Q[s, a] += alpha * (td_target - Q[s, a])\n",
        "\n",
        "            timestep += 1\n",
        "            s, a = s_next, a_next\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "    policy_greedy = np.argmax(Q, axis=1)\n",
        "    env.close()\n",
        "    return Q, policy_greedy\n",
        "\n",
        "# ---------------------------\n",
        "# Quick demo (optional)\n",
        "# ---------------------------\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # TD(0) prediction under a random policy\n",
        "    V = td0_policy_evaluation(\n",
        "        env_id=\"FrozenLake-v1\",\n",
        "        is_slippery=True,   # set False for deterministic grid\n",
        "        gamma=0.99,\n",
        "        alpha=0.1,\n",
        "        epsilon_random_policy=1.0,\n",
        "        episodes=25_000,\n",
        "        seed=42\n",
        "    )\n",
        "    print(\"TD(0) Value function estimate (V):\")\n",
        "    print(V.reshape(4, 4))  # for 4x4 FrozenLake\n",
        "\n",
        "    # SARSA control to learn a policy\n",
        "    Q, pi = sarsa_control(\n",
        "        env_id=\"FrozenLake-v1\",\n",
        "        is_slippery=True,\n",
        "        gamma=0.99,\n",
        "        alpha=0.1,\n",
        "        epsilon_start=1.0,\n",
        "        epsilon_end=0.05,\n",
        "        epsilon_decay_steps=80_000,\n",
        "        episodes=120_000,\n",
        "        max_steps_per_ep=200,\n",
        "        seed=7\n",
        "    )\n",
        "    print(\"\\nGreedy policy from SARSA (actions 0:Left, 1:Down, 2:Right, 3:Up):\")\n",
        "    print(pi.reshape(4, 4))\n",
        "    print(\"\\nState-action values Q[s,a] (reshaped per action for readability):\")\n",
        "    for a in range(Q.shape[1]):\n",
        "        print(f\"Action {a}:\")\n",
        "        print(Q[:, a].reshape(4, 4))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sSg55clRueW1",
        "outputId": "7b4121a5-a6e8-47b8-a1fa-46f1a08b59c0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TD(0) Value function estimate (V):\n",
            "[[0.01154136 0.00756639 0.02747178 0.013248  ]\n",
            " [0.01991066 0.         0.0626995  0.        ]\n",
            " [0.03684871 0.0758343  0.18880016 0.        ]\n",
            " [0.         0.16529731 0.52072376 0.        ]]\n",
            "\n",
            "Greedy policy from SARSA (actions 0:Left, 1:Down, 2:Right, 3:Up):\n",
            "[[0 3 1 3]\n",
            " [0 0 2 0]\n",
            " [3 1 0 0]\n",
            " [0 2 1 0]]\n",
            "\n",
            "State-action values Q[s,a] (reshaped per action for readability):\n",
            "Action 0:\n",
            "[[0.35879099 0.256256   0.2354679  0.12917957]\n",
            " [0.40523588 0.         0.15157585 0.        ]\n",
            " [0.2666601  0.318245   0.56247467 0.        ]\n",
            " [0.         0.30634394 0.63039223 0.        ]]\n",
            "Action 1:\n",
            "[[0.31427124 0.19830804 0.24873389 0.11229348]\n",
            " [0.23471058 0.         0.169121   0.        ]\n",
            " [0.26211337 0.56930223 0.30913391 0.        ]\n",
            " [0.         0.38829745 0.85720356 0.        ]]\n",
            "Action 2:\n",
            "[[0.2981604  0.10287446 0.2361989  0.14233501]\n",
            " [0.24663785 0.         0.21868749 0.        ]\n",
            " [0.25617631 0.40590125 0.30485473 0.        ]\n",
            " [0.         0.70056179 0.62130542 0.        ]]\n",
            "Action 3:\n",
            "[[0.31968015 0.26972596 0.23349976 0.22772739]\n",
            " [0.29724172 0.         0.05580837 0.        ]\n",
            " [0.47997678 0.27368312 0.16935533 0.        ]\n",
            " [0.         0.49462199 0.63985899 0.        ]]\n"
          ]
        }
      ]
    }
  ]
}