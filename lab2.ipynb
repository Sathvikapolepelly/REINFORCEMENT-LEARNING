{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM9fMnLsfnZRYcPt5PkneX9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sathvikapolepelly/REINFORCEMENT-LEARNING/blob/main/lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Tm1r3VTGzBzv"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import random\n",
        "from typing import Dict, Tuple, List\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def make_epsilon_greedy_policy(Q: Dict, nA: int, epsilon: float):\n",
        "    \"\"\"Return a policy function that takes state and returns action probabilities.\"\"\"\n",
        "    def policy_fn(state):\n",
        "        probs = np.ones(nA) * (epsilon / nA)\n",
        "        best_a = np.argmax(Q[state])\n",
        "        probs[best_a] += (1.0 - epsilon)\n",
        "        return probs\n",
        "    return policy_fn\n",
        "\n",
        "def generate_episode(env, policy):\n",
        "    \"\"\"Generate an episode: returns list of (state, action, reward).\"\"\"\n",
        "    episode = []\n",
        "    state, _ = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        probs = policy(state)\n",
        "        action = np.random.choice(len(probs), p=probs)\n",
        "        next_state, reward, terminated, truncated, _ = env.step(action)\n",
        "        episode.append((state, action, reward))\n",
        "        state = next_state\n",
        "        done = terminated or truncated\n",
        "    return episode\n"
      ],
      "metadata": {
        "id": "eFdsHVgr1TOy"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def first_visit_mc_policy_evaluation(env, policy_fn, gamma=1.0, num_episodes=500000):\n",
        "    returns_sum = defaultdict(float)\n",
        "    returns_count = defaultdict(float)\n",
        "    V = defaultdict(float)\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 10000 == 0:\n",
        "            print(f\"Episode {i_episode}/{num_episodes}\")\n",
        "\n",
        "        episode = generate_episode(env, policy_fn)\n",
        "        visited_states = set()\n",
        "\n",
        "        for t, (state, _, _) in enumerate(episode):\n",
        "            if state not in visited_states:\n",
        "                visited_states.add(state)\n",
        "                G = sum([r * (gamma ** i) for i, (_, _, r) in enumerate(episode[t:])])\n",
        "                returns_sum[state] += G\n",
        "                returns_count[state] += 1.0\n",
        "                V[state] = returns_sum[state] / returns_count[state]\n",
        "    return V\n",
        "\n",
        "def mc_control_epsilon_greedy(env, num_episodes=500000, gamma=1.0, epsilon=0.1):\n",
        "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
        "\n",
        "    for i_episode in range(1, num_episodes + 1):\n",
        "        if i_episode % 10000 == 0:\n",
        "            print(f\"Episode {i_episode}/{num_episodes}\")\n",
        "\n",
        "        policy = make_epsilon_greedy_policy(Q, env.action_space.n, epsilon)\n",
        "        episode = generate_episode(env, policy)\n",
        "        visited_state_actions = set()\n",
        "\n",
        "        for t, (state, action, _) in enumerate(episode):\n",
        "            if (state, action) not in visited_state_actions:\n",
        "                visited_state_actions.add((state, action))\n",
        "                G = sum([r * (gamma ** i) for i, (_, _, r) in enumerate(episode[t:])])\n",
        "                Q[state][action] += 0.01 * (G - Q[state][action])  # constant Î±=0.01\n",
        "    return Q\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    env = gym.make(\"Blackjack-v1\", sab=True)"
      ],
      "metadata": {
        "id": "MBm0wrE61iHx"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    def simple_policy(state):\n",
        "        score, dealer, usable_ace = state\n",
        "        return np.array([1.0, 0.0]) if score >= 20 else np.array([0.0, 1.0])\n",
        "\n",
        "    V = first_visit_mc_policy_evaluation(env, simple_policy, num_episodes=100000)\n",
        "    print(\"Policy Value Estimates for simple_policy:\")\n",
        "    print(list(V.items())[:10])\n",
        "\n",
        "    # Example: MC Control to learn optimal policy\n",
        "    Q = mc_control_epsilon_greedy(env, num_episodes=500000)\n",
        "    optimal_policy = {state: np.argmax(actions) for state, actions in Q.items()}\n",
        "    print(\"Sample learned policy entries:\")\n",
        "    print(list(optimal_policy.items())[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_k3uRc516eS",
        "outputId": "f883b2af-77b8-4e0f-8186-4daef359cc99"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 10000/100000\n",
            "Episode 20000/100000\n",
            "Episode 30000/100000\n",
            "Episode 40000/100000\n",
            "Episode 50000/100000\n",
            "Episode 60000/100000\n",
            "Episode 70000/100000\n",
            "Episode 80000/100000\n",
            "Episode 90000/100000\n",
            "Episode 100000/100000\n",
            "Policy Value Estimates for simple_policy:\n",
            "[((20, 3, 0), 0.616135328562134), ((21, 4, 1), 0.9761904761904762), ((11, 8, 0), -0.03529411764705882), ((12, 8, 0), -0.5407035175879397), ((17, 8, 0), -0.6569821930646673), ((20, 10, 0), 0.43704680290046144), ((19, 2, 0), -0.7127272727272728), ((10, 9, 0), -0.1095890410958904), ((21, 9, 1), 0.9910913140311804), ((10, 10, 0), -0.21257861635220127)]\n",
            "Episode 10000/500000\n",
            "Episode 20000/500000\n",
            "Episode 30000/500000\n",
            "Episode 40000/500000\n",
            "Episode 50000/500000\n",
            "Episode 60000/500000\n",
            "Episode 70000/500000\n",
            "Episode 80000/500000\n",
            "Episode 90000/500000\n",
            "Episode 100000/500000\n",
            "Episode 110000/500000\n",
            "Episode 120000/500000\n",
            "Episode 130000/500000\n",
            "Episode 140000/500000\n",
            "Episode 150000/500000\n",
            "Episode 160000/500000\n",
            "Episode 170000/500000\n",
            "Episode 180000/500000\n",
            "Episode 190000/500000\n",
            "Episode 200000/500000\n",
            "Episode 210000/500000\n",
            "Episode 220000/500000\n",
            "Episode 230000/500000\n",
            "Episode 240000/500000\n",
            "Episode 250000/500000\n",
            "Episode 260000/500000\n",
            "Episode 270000/500000\n",
            "Episode 280000/500000\n",
            "Episode 290000/500000\n",
            "Episode 300000/500000\n",
            "Episode 310000/500000\n",
            "Episode 320000/500000\n",
            "Episode 330000/500000\n",
            "Episode 340000/500000\n",
            "Episode 350000/500000\n",
            "Episode 360000/500000\n",
            "Episode 370000/500000\n",
            "Episode 380000/500000\n",
            "Episode 390000/500000\n",
            "Episode 400000/500000\n",
            "Episode 410000/500000\n",
            "Episode 420000/500000\n",
            "Episode 430000/500000\n",
            "Episode 440000/500000\n",
            "Episode 450000/500000\n",
            "Episode 460000/500000\n",
            "Episode 470000/500000\n",
            "Episode 480000/500000\n",
            "Episode 490000/500000\n",
            "Episode 500000/500000\n",
            "Sample learned policy entries:\n",
            "[((16, 10, 0), np.int64(1)), ((13, 10, 1), np.int64(1)), ((9, 10, 0), np.int64(1)), ((9, 6, 0), np.int64(1)), ((16, 3, 0), np.int64(0)), ((21, 1, 1), np.int64(0)), ((17, 1, 0), np.int64(1)), ((14, 3, 0), np.int64(0)), ((11, 4, 0), np.int64(1)), ((15, 4, 0), np.int64(0))]\n"
          ]
        }
      ]
    }
  ]
}